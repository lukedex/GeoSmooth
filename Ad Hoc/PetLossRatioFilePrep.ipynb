{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used directories:\n",
    "\n",
    "#### Finance file locations:\n",
    "'\\\\\\\\sigltd.co.uk\\SIG_net\\Departments\\Protection\\Pet\\Loss Ratios\\March 2024'\n",
    "\n",
    "#### Postcode data (can be moved to Finance dir)\n",
    "'\\\\\\\\coveainsurance.co.uk\\data\\Shared Data\\BI Reporting\\BI Function\\Power BI\\\\2. Offline Data Sources'\n",
    "\n",
    "#### Claims postcode lookup:\n",
    "'\\\\\\\\sigltd.co.uk\\SIG_net\\Departments\\Protection\\Pet\\Loss Ratios\\MDA File prep'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta #to add months to date\n",
    "import re\n",
    "# for converting excel date number to date\n",
    "import xlrd \n",
    "\n",
    "# for testing function performance\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# for claims postcode\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '\\\\\\\\sigltd.co.uk\\SIG_net\\Departments\\Protection\\Pet\\Loss Ratios\\March 2024'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if file is an Excel file and meets the name condition\n",
    "    if filename.endswith('.xlsx') and not filename == 'Overall Loss ratio.xlsx':                  # and filename != 'Overall Loss ratio.xlsx'\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        #print(filename, time.ctime(os.path.getmtime(file_path)))\n",
    "        print(filename, datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}                                                                                 #Better to use a name such as dataframes_dict \n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if file is an Excel file and meets the name condition\n",
    "    if filename.endswith('.xlsx') and not filename == 'Overall Loss ratio.xlsx':                #Repeating above\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(filename)\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        # Iterate over all sheets in the Excel file\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            sheet_name_lower = str.lower(sheet_name)\n",
    "            # Check if sheet name meets the condition\n",
    "            if sheet_name_lower.endswith('data') or \\\n",
    "                'fees' in sheet_name_lower or \\\n",
    "                (filename.startswith('IYP Written Data') and sheet_name == 'Sheet1'):\n",
    "                # Load the data from the sheet into a dataframe\n",
    "                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
    "                # Check if the sheet is empty\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                # Check if the first cell is empty and use the appropriate row for headers\n",
    "                headers = df.iloc[1] if pd.isnull(df.iloc[0, 0]) else df.iloc[0]\n",
    "                df = pd.DataFrame(df.values[2:], columns=headers) if pd.isnull(df.iloc[0, 0]) else pd.DataFrame(df.values[1:], columns=headers)\n",
    "                # Add file name and date modified to df\n",
    "                df['filename'] = filename\n",
    "                df['filemodified'] = datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                # Store the dataframe in the dictionary\n",
    "                dataframes[filename.replace('.xlsx', '') + '_' + sheet_name] = df\n",
    "                print(filename, sheet_name, len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableSchemaPath = '\\\\\\\\sigltd.co.uk\\SIG_net\\Departments\\Protection\\Pet\\Loss Ratios\\MDA File prep\\PetLossRatio_TableSchema.csv'\n",
    "\n",
    "tableSchema = pd.read_csv(tableSchemaPath, header = 0)\n",
    "\n",
    "colsToDrop = tableSchema[tableSchema.Exclude == True]['ModelColumnName'].tolist()\n",
    "\n",
    "# Create a dictionary for column name transformations\n",
    "transform_dict = pd.Series(tableSchema.ModelColumnName.values, index=tableSchema.ColumnName).to_dict()\n",
    "\n",
    "# Create a list of columns to exclude\n",
    "exclude_list = tableSchema.loc[tableSchema['Exclude'] == True, 'ModelColumnName'].unique().tolist()\n",
    "\n",
    "# Create a list of columns to convert to date\n",
    "dateColumns = tableSchema[(tableSchema.DataType == 'date')][\"ModelColumnName\"].tolist()\n",
    "dateColumns = list(set(dateColumns))\n",
    "\n",
    "# Create list of Dimension columns to replace null with UNKNOWN\n",
    "colsCleanForDim = tableSchema[\n",
    "    (tableSchema.CleanForDim == True) & \n",
    "    (tableSchema.DataType != 'date' )]['ModelColumnName'].tolist()\n",
    "\n",
    "\n",
    "def fnTransformHeaders(df):\n",
    "    # Rename the columns using the transformation dictionary\n",
    "    df.rename(columns=transform_dict, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fnRemoveColumns(df):\n",
    "    # Remove the columns in the exclude list\n",
    "    df.drop(columns=exclude_list, errors='ignore', inplace=True)\n",
    "    return df\n",
    "\n",
    "def fnPrepDimCols(df):\n",
    "    for col in df.columns:\n",
    "        if col in colsCleanForDim:\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: 'UNKNOWN' if pd.isnull(x) or x == '' else x)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def convert_string_to_date(input_string):\n",
    "    #check for excel number dates:\n",
    "    if type(input_string) == int:\n",
    "        return datetime.datetime(1899, 12, 30) + datetime.timedelta(days=int(input_string))\n",
    "    #check different date formats:\n",
    "    formats = [\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d-%m-%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%d.%m.%Y\",\n",
    "        \"%d/%m.%Y\"\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.datetime.strptime(str(input_string), fmt)\n",
    "            return date_obj\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    #known issues\n",
    "    errorDate =  '00/01/2020','02/012022','02/09/201807/2019','13/111/2019','14/06+/2021','14/12/20190','21/10/20189','25/17/2020','27/07/20221','27/11/20201','29/02/2019','29/02/2021','29/02/2022','30/042021','30/09/20232','31/02/2022','31/04/2019','31/04/2022','31/04/2023','31/06/2019','31/06/2020','31/06/2023','31/09/2021','31/09/2023','31/11/2020','31/11/2023','31/17/2021','65/03/2022','22/09/202','05/09/023','23/09/202'\n",
    "    fixedDate = '2020-01-01','2022-01-02','2018-09-02','2019-11-13','2021-06-14','2019-12-14','2018-10-21','2020-10-25','2021-07-27','2020-11-27','2019-02-28','2021-02-28','2022-02-28','2021-04-30','2023-09-30','2022-02-28','2019-04-30','2022-04-30','2023-04-30','2019-06-30','2023-06-30','2023-06-30','2021-09-30','2023-09-30','2020-11-30','2023-11-30','2021-07-31','2022-03-16','2022-09-22','2023-09-05','2022-09-23'\n",
    "    fixedDate = [datetime.datetime.strptime(d, '%Y-%m-%d') for d in fixedDate]\n",
    "    \n",
    "    if str(input_string) in errorDate:\n",
    "        return fixedDate[errorDate.index(input_string)]\n",
    "        \n",
    "    return None\n",
    "\n",
    "def fnConvertDates(df):\n",
    "    for col in df.columns:\n",
    "        if col in dateColumns:\n",
    "            print('Converting dates:', key, col)\n",
    "            df.loc[:, col] = df.loc[:, col].apply(convert_string_to_date)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fnConvertDates(df):\n",
    "    for col in df.columns:\n",
    "        if col in dateColumns:\n",
    "            print('Converting dates:', key, col)\n",
    "            df.loc[:, col] = df.loc[:, col].apply(convert_string_to_date)\n",
    "    return df\n",
    "\n",
    "def fnIntersectCols(df, columns):\n",
    "    df_cols = [val for val in df.columns if val in columns]\n",
    "    return df_cols\n",
    "\n",
    "### Setup save func to save in notebook directory\n",
    "def write_df_to_csv(df, filename):\n",
    "    df.to_csv(filename, encoding = 'utf-8-sig', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if will work faster of fnConvertDates\n",
    "#for key, df in dataframes.items():\n",
    "#    df_dateColumns = [ val for val in dateColumns if val in df.columns]\n",
    "#    df.loc[:, df_dateColumns] = df.loc[:, df_dateColumns].apply(convert_string_to_date\n",
    "#    print(key, df_dateColumns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format headers, remove columns, format dates, remove rows where reporting_month is null\n",
    "colsToInt = list(set(tableSchema[tableSchema.DataType == 'int']['ModelColumnName']))\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Transform headers\n",
    "    df = fnTransformHeaders(df)\n",
    "\n",
    "    # Remove columns\n",
    "    df = fnRemoveColumns(df)\n",
    "\n",
    "    # Convert/clean dates\n",
    "    df = fnConvertDates(df)\n",
    "    \n",
    "    # Replace empty values with NaN\n",
    "    df['reporting_month'].replace('', np.nan, inplace=True)\n",
    "    \n",
    "    # Drop rows with null values in the reporting_month column\n",
    "    df.dropna(subset=['reporting_month'], inplace=True)\n",
    "\n",
    "    # Format nulls on columns to be used as dimensions\n",
    "    df = fnPrepDimCols(df)\n",
    "    \n",
    "    # Fix int on YOA and YOL\n",
    "\n",
    "    dfCols = fnIntersectCols(df, colsToInt)\n",
    "    df.loc[:,dfCols] = df.loc[:,dfCols].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check for files with no affinity\n",
    "for key, df in dataframes.items():\n",
    "    if 'affinity' not in df.columns:\n",
    "        print(key)\n",
    "        #[affinityAll.add(x) for x in affinityDf]\n",
    "        #df[key].loc[:, 'affinity'] = 'INSURE YOUR PAWS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataframe with missing affinity\n",
    "\n",
    "dfma = 'IYP Written Data_Sheet1'\n",
    "dataframes[dfma]['affinity'] = 'IYP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get clean list of affinities for dim\n",
    "\n",
    "affinityAll = set()\n",
    "affinityDf = set()\n",
    "for key, df in dataframes.items():\n",
    "    if 'affinity' in df.columns:\n",
    "        affinityDf = set(df.affinity.tolist())\n",
    "        [affinityAll.add(x) for x in affinityDf]\n",
    "        #df[key].loc[:, 'affinity'] = 'INSURE YOUR PAWS'\n",
    "        \n",
    "print(affinityAll)\n",
    "\n",
    "\n",
    "affinityAllclean = [x.upper().replace('-','').replace('  ', ' ') for x in affinityAll]\n",
    "affinityAllclean = [x.replace('VETSMEDICOVER', 'VMC') for x in affinityAllclean]\n",
    "affinityAllclean = [x.replace('INSURE YOUR PAWS', 'IYP') for x in affinityAllclean]\n",
    "affinityAllclean = [x.replace('LIFETIME PET COVER', 'LIFETIME') for x in affinityAllclean]\n",
    "affinityAllclean = [x.replace('LPC', 'LIFETIME') for x in affinityAllclean]\n",
    "\n",
    "affinityName = []\n",
    "affinityTPL = []\n",
    "\n",
    "for a in affinityAllclean:\n",
    "    if 'TPL' in a:\n",
    "        affinityName.append(a.replace('TPL','').strip())\n",
    "        affinityTPL.append('TPL')\n",
    "    else:\n",
    "        affinityName.append(a.strip())\n",
    "        affinityTPL.append('NonTPL')\n",
    "\n",
    "affinities = pd.DataFrame({\n",
    "    'affinity': list(affinityAll),\n",
    "    #'affinity_clean': affinityAllclean,\n",
    "    'affinity_name': affinityName,\n",
    "    'affinity_tpl': affinityTPL\n",
    "})\n",
    "\n",
    "\n",
    "affinities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VMC Written and Paid data in single table, requires splitting\n",
    "\n",
    "vmc_premium = 'VMC Loss Ratio Data_Premium Data'\n",
    "vmc_df = ''\n",
    "n = 0\n",
    "for key, df in dataframes.items():\n",
    "    if key.endswith(vmc_premium):\n",
    "        vmc_df = key\n",
    "        n += 1\n",
    "if n == 1:\n",
    "    VMCpremium = dataframes[vmc_df]\n",
    "    dataframes[vmc_df + '_Written'] = VMCpremium[VMCpremium.transaction_group == 'Written']\n",
    "    dataframes[vmc_df + '_Paid'] = VMCpremium[VMCpremium.transaction_group == 'Paid']\n",
    "else:\n",
    "    print('multiple vmc premium tables!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written to 'Earned' based on Reporting Month\n",
    "\n",
    "Where paid data is not available, written data is exploded into pseudo-earned.\n",
    "Replicating logic from finance reporting.\n",
    "Change to Earned GWP would require converting all premium, and abandoning use of paid.\n",
    "\n",
    "Rules applied:\n",
    "\n",
    "Split written dataframe into two sets:\n",
    "*  <b>Keep as is</b>: YEAR(Reporting Month) > YOA -- Assuming cancellations, not to be earned out\n",
    "*  <b>Expanded</b>: YEAR(Reporting Month) <= YOA -- Assuming NB/RNL, to earn out\n",
    "\n",
    "#### Expanded\n",
    "For each reporting month create list of current and Next 11 months.\n",
    "Join on reporting month\n",
    "Multiply all columns listed as currency type in TableSchema by 1/12\n",
    "\n",
    "#### Combine tables 'Keep as is' and 'Expanded'\n",
    "\n",
    "### Data requiring converting:\n",
    "\n",
    "#### THISTLE\n",
    "Base logic using Written data\n",
    "\n",
    "#### VMC\n",
    "All written premium where reporting month is on or before 2022-07-01 and collector is 'Premium Credit'\n",
    "Original VMC BDX file combines both paid and written, it is split out, but for consistency with THISTLE I'm using the \\__Premium Data_ file\n",
    "\n",
    "#### LIFETIME\n",
    "At the moment all written data has to be converted, with no cap on YEAR(reporting month) <= YOA.\n",
    "Likely to be reviewed in July\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnDivideCurrForEarned(df, n):\n",
    "    #get all currency cols\n",
    "    try:\n",
    "        currCols = tableSchema[ (tableSchema.DataType == 'currency') & ( tableSchema.Exclude == False )]['ModelColumnName'].tolist()\n",
    "    except ValueError:\n",
    "        'is tableSchema loaded?'\n",
    "        return\n",
    "    #get currency cols in df\n",
    "    df_currCols = [val for val in df.columns if val in currCols]\n",
    "    df.loc[:, df_currCols] = df.loc[:, df_currCols].div(n)\n",
    "    return\n",
    "\n",
    "def fnExpandOnReportingMonth(df):\n",
    "    \n",
    "    if 'reporting_month' not in df.columns:\n",
    "        print('no reporting_month column')\n",
    "        return\n",
    "    \n",
    "    expandby = pd.DataFrame(set(df.reporting_month), columns = [\"reporting_month\"])\n",
    "    expandby['key'] = 0\n",
    "\n",
    "    expand_df = pd.DataFrame([x for x in range(12)], columns = ['months'])\n",
    "    expand_df['key'] = 0\n",
    "\n",
    "    expandby = expandby.merge(expand_df, on = 'key', how = 'outer')\n",
    "    expandby['earned_month'] = expandby.apply(\n",
    "        lambda row: row.reporting_month + relativedelta(months = row.months),\n",
    "        axis = 1)\n",
    "                                    \n",
    "    expandby = expandby.drop(['key','months'], axis = 1)\n",
    "\n",
    "    df = df.merge(expandby, on = 'reporting_month', how = 'inner')\n",
    "    df = df.drop(['reporting_month'], axis = 1)\n",
    "    df = df.rename(columns={'earned_month': 'reporting_month'})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fnGetDataForEarned(endswith, contains):\n",
    "    n_df = [x for x in dataframes.keys() if (x.endswith(endswith)) and (contains in x.upper())]\n",
    "    if len(n_df) == 1:\n",
    "        df_w = dataframes[n_df[0]]\n",
    "    else:\n",
    "        print(f'this many {len(n_df)} vmc dataframes matching') \n",
    "        return None\n",
    "    return df_w\n",
    "    \n",
    "#learn how to use classes\n",
    "def fnGetKeyForEarned(endswith, contains):\n",
    "    n_df = [x for x in dataframes.keys() if (x.endswith(endswith)) and (contains in x.upper())]\n",
    "    if len(n_df) == 1:\n",
    "        return n_df[0]\n",
    "    else:\n",
    "        print(f'this many {len(n_df)} vmc dataframes matching')                                           \n",
    "        return None\n",
    "    \n",
    "                                             \n",
    "                                             \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get written files to update\n",
    "\n",
    "# VMC\n",
    "vmc_w = fnGetDataForEarned('_Premium Data_Written', 'VMC')\n",
    "vmc_w = vmc_w[vmc_w.collector == 'Premium Credit']\n",
    "\n",
    "# THISTLE\n",
    "th_w = fnGetDataForEarned('_Written Premium Data', 'THISTLE')\n",
    "\n",
    "# LIFETIME\n",
    "lt_w = fnGetDataForEarned('_Premium Data', 'LIFETIME')\n",
    "\n",
    "print(f'VMC rows: ', len(vmc_w))\n",
    "print(f'THISTLE rows: ', len(vmc_w))\n",
    "print(f'LIFETIME rows: ', len(lt_w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand VMC\n",
    "vmc_cutoff = datetime.datetime(2022,7,1)\n",
    "\n",
    "#vmc_cutoff + relativedelta(months=1)\n",
    "#set(vmc_w.yoa)\n",
    "#[x.year for x in list(set(vmc_w.reporting_month))]\n",
    "\n",
    "vmc_expand = vmc_w[\n",
    "    ( vmc_w.yoa == [ x.year for x in vmc_w.reporting_month] ) & \n",
    "    ( vmc_w.reporting_month <= vmc_cutoff ) \n",
    "]\n",
    "\n",
    "vmc_keep = vmc_w[\n",
    "    ( vmc_w.yoa != [ x.year for x in vmc_w.reporting_month] ) |\n",
    "    ( vmc_w.reporting_month > vmc_cutoff ) \n",
    "]\n",
    "\n",
    "vmc_expand = fnExpandOnReportingMonth(vmc_expand)\n",
    "\n",
    "fnDivideCurrForEarned(vmc_expand, 12)\n",
    "\n",
    "vmc_earned = pd.concat([vmc_keep, vmc_expand])\n",
    "\n",
    "vmc_earned.transaction_group = 'Earned'\n",
    "\n",
    "# Check\n",
    "print(sum(vmc_earned.allianz_net))\n",
    "print(sum(vmc_w.allianz_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIFETIME\n",
    "### temporarily explode all. to be confirmed\n",
    "\n",
    "lt_expand = lt_w\n",
    "\n",
    "lt_expand = fnExpandOnReportingMonth(lt_expand)\n",
    "\n",
    "fnDivideCurrForEarned(lt_expand, 12)\n",
    "\n",
    "lt_earned = lt_expand\n",
    "\n",
    "lt_earned.transaction_group = 'Earned'\n",
    "\n",
    "# Check\n",
    "print(sum(lt_earned.allianz_net))\n",
    "print(sum(lt_w.allianz_net))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THISTLE\n",
    "\n",
    "thistle_expand = th_w[\n",
    "    ( th_w.yoa == [ x.year for x in th_w.reporting_month] )\n",
    "]\n",
    "\n",
    "thistle_keep = th_w[\n",
    "    ( th_w.yoa != [ x.year for x in th_w.reporting_month] )\n",
    "]\n",
    "\n",
    "thistle_expand = fnExpandOnReportingMonth(thistle_expand)\n",
    "\n",
    "fnDivideCurrForEarned(thistle_expand, 12)\n",
    "\n",
    "thistle_earned = pd.concat([thistle_keep, thistle_expand])\n",
    "\n",
    "thistle_earned.transaction_group = 'Earned'\n",
    "\n",
    "# Check\n",
    "print(sum(th_w.allianz_net), len(th_w))\n",
    "print(sum(thistle_earned.allianz_net), len(thistle_earned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add VMC Earned to df dict\n",
    "vmc_earned_name = fnGetKeyForEarned('_Premium Data_Written', 'VMC').replace('_Premium Data_Written', '_Earned Premium Data')\n",
    "dataframes[vmc_earned_name] = vmc_earned\n",
    "\n",
    "### Add LIFETIME Earned to df dict\n",
    "lifetime_earned_name = fnGetKeyForEarned('_Premium Data', 'LIFETIME').replace('_Premium Data', '_Earned Premium Data')\n",
    "dataframes[lifetime_earned_name] = lt_earned\n",
    "\n",
    "### Add THISTLE Earned to df dict\n",
    "thistle_earned_name = fnGetKeyForEarned('_Written Premium Data', 'THISTLE').replace('_Written Premium Data', '_Earned Premium Data')\n",
    "dataframes[thistle_earned_name] = thistle_earned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create mapping table to combine written, paid, and claims tables\n",
    "\n",
    "concat_check = ['written','claims','paid','fees','Lifetime Pet Cover Loss Ratio Data_Premium Data', 'earned']\n",
    "concat_file = ['written','claims','paid','fees','written','paid']\n",
    "concat_df = pd.DataFrame(columns=['key','file'])\n",
    "for key, df in dataframes.items():\n",
    "    for n in range(len(concat_check)):\n",
    "        if str.lower(concat_check[n]) in str.lower(key):\n",
    "            concat_df = concat_df.append({'key': key, 'file': concat_file[n]}, ignore_index = True)\n",
    "    if not (key in concat_df.key.values):\n",
    "        concat_df = concat_df.append({'key': key, 'file': 'other'}, ignore_index = True)\n",
    "        \n",
    "\n",
    "concat_df.sort_values(by = ['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all written, paid, and claims tables\n",
    "\n",
    "dataframes_out = {}\n",
    "dfs = list(set(concat_file))\n",
    "for c in dfs:\n",
    "    dfs_list = concat_df[concat_df.file == c]['key'].tolist()\n",
    "    dfs_concat = pd.concat([dataframes[i] for i in dfs_list])\n",
    "    dataframes_out[c] = dfs_concat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add affinity_name (all) and affinity_tpl (claims only)\n",
    "\n",
    "for key, df in dataframes_out.items():\n",
    "    if key == 'claims':\n",
    "        dataframes_out[key] = df.merge(affinities, on = 'affinity', how = 'inner')\n",
    "    else:\n",
    "        dataframes_out[key]  = df.merge(affinities[['affinity','affinity_name']], on = 'affinity', how = 'inner')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check affinity\n",
    "test1 = pd.DataFrame()\n",
    "for key, df in dataframes_out.items():\n",
    "    if key == 'claims':\n",
    "        dfa = df[['affinity','affinity_name','affinity_tpl']].copy()\n",
    "    else:\n",
    "        dfa = df[['affinity','affinity_name']].copy()\n",
    "    dfa.drop_duplicates(inplace = True)\n",
    "    dfa.loc[:,'key'] = key\n",
    "    test1 = pd.concat([test1, dfa], axis = 0, ignore_index = True)\n",
    "    \n",
    "#test2 = test1[['affinity_name']].copy()\n",
    "\n",
    "pivot_test = test1.pivot_table(index = 'affinity_name', columns='key', values='affinity', aggfunc='count')\n",
    "pivot_test['TPL'] = test1[test1.affinity_tpl == 'TPL'].groupby('affinity_name')['affinity_tpl'].count()\n",
    "pivot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add policyNkey. Based on policy_no + affinity_name + yoa\n",
    "def fnLambdaPolicyNkey(policy_no, affinity_name, yoa):\n",
    "    if pd.isna(policy_no) or pd.isna(affinity_name) or pd.isna(yoa):\n",
    "        return 'UNDEFINED'\n",
    "    elif str(policy_no).lower().strip() == 'bdx':\n",
    "        return 'UNDEFINED'\n",
    "    else:\n",
    "        return '+'.join([str(policy_no).strip(),str(affinity_name),str(yoa)])\n",
    "\n",
    "\n",
    "def fnAddPolicyNkey(df):\n",
    "    policyNkeyCols = ['policy_no','affinity_name','yoa']\n",
    "    key_cols = fnIntersectCols(df, policyNkeyCols)\n",
    "    if len(key_cols) != len(policyNkeyCols):\n",
    "        df.loc[:, 'policyNkey'] = 'UNDEFINED'\n",
    "    else:\n",
    "        df.loc[:, 'policyNkey'] = df.apply(lambda x: fnLambdaPolicyNkey(x.policy_no, x.affinity_name, x.yoa), axis = 1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dataframes_out.items():\n",
    "    fnAddPolicyNkey(df)\n",
    "    print(key, df.policyNkey[:3].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add developmentID\n",
    "\n",
    "#def fnAddDevelopmentID(df):\n",
    "#    df.loc[:, 'developmentid'] = df.apply(lambda x: )\n",
    " \n",
    "def fnLambdaDevelopmentID(yoa, reporting_month):\n",
    "    if yoa is None or yoa > 2100:\n",
    "        return -1\n",
    "    else:\n",
    "        return yoa * 1000 + ( reporting_month.year - yoa ) * 12 + reporting_month.month\n",
    "    \n",
    "def fnAddDevelopmentID(df):\n",
    "    df.loc[:, 'developmentid'] = df.apply(lambda x: fnLambdaDevelopmentID(x.yoa, x.reporting_month), axis = 1)\n",
    "\n",
    "#dataframes_out['written'].apply(lambda x: fnLambdaDevelopmentID(x.yoa, x.reporting_month), axis = 1)\n",
    "\n",
    "for key, df in dataframes_out.items():\n",
    "    fnAddDevelopmentID(df)\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pet age\n",
    "\n",
    "def calculate_age(born, asat):\n",
    "    return asat.year - born.year - ((asat.month, asat.day) < (born.month, born.day))\n",
    "\n",
    "\n",
    "# If pet_dob is null: return -1\n",
    "# If start_date is available: calculate the age in years based on the difference\n",
    "# If not, use YOA start date\n",
    "# If age is negative or null: return -1\n",
    "\n",
    "def fnReturnPetAge(pet_dob, start_date, yoa):\n",
    "    age = -1\n",
    "    if pd.isnull(pet_dob):\n",
    "        return age\n",
    "    elif not pd.isnull(start_date):\n",
    "        age = calculate_age(pet_dob, start_date)\n",
    "    else:\n",
    "        age = calculate_age(pet_dob, datetime.date(yoa, 1, 1))\n",
    "\n",
    "        \n",
    "    if age < 0 or pd.isnull(age):\n",
    "        return -1   \n",
    "    else:\n",
    "        return age\n",
    "    \n",
    "    \n",
    "ageCalcList = ['pet_dob','start_date','yoa']\n",
    "\n",
    "for key, df in dataframes_out.items():\n",
    "    if 'pet_dob' in df.columns:\n",
    "        print(key, fnIntersectCols(df, ageCalcList))\n",
    "        df.loc[:, 'pet_age'] = df.apply(lambda x: fnReturnPetAge(x.pet_dob, x.start_date, x.yoa), axis = 1 )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Pet Type\n",
    "# Format Breed: add dictionary \n",
    "# Add pet key (petNkey = pet_type + breed)\n",
    "# Clean sales channel\n",
    "\n",
    "def fnFormatDimText(value):\n",
    "    if pd.isna(value) or value == 0:\n",
    "        value = 'Unknown'\n",
    "    value = value.title().strip()\n",
    "    return value\n",
    "\n",
    "breedDict = {\n",
    "    'Labrador': 'Labrador Retriever',\n",
    "    'Labrador (Retriever)': 'Labrador Retriever',\n",
    "    'Golden Labrador': 'Golden Labrador Retriever',\n",
    "    'Chocolate Labrador' : 'Labrador Retriever',\n",
    "    'Retriever (Labrador)': 'Labrador Retriever',\n",
    "    'Retriever (Labrador)': 'Labrador Retriever',\n",
    "    'Labrador (Retriever) Cross': 'Labrador Retriever Cross',\n",
    "    'Non Pedigree Rabbit': 'Non-Pedigree',\n",
    "    'Non-Pedigree (Cat)': 'Non-Pedigree',\n",
    "    'Non-Pedigree (Dog)': 'Non-Pedigree'    \n",
    "}\n",
    "\n",
    "\n",
    "def fnFormatPetBreed(breed):\n",
    "    breed = fnFormatDimText(breed)\n",
    "    if breed in breedDict:\n",
    "        breed = breedDict[breed]\n",
    "    return breed\n",
    "\n",
    "for key, df in dataframes_out.items():\n",
    "    petNkeylist = ['pet_type','breed']\n",
    "    \n",
    "    if 'pet_type' in df.columns:\n",
    "        df.pet_type = [ fnFormatDimText(x) for x in df.pet_type ]\n",
    "    if 'breed' in df.columns:\n",
    "        df.breed = [ fnFormatPetBreed(x) for x in df.breed ]\n",
    "    \n",
    "    if all(e in df.columns for e in petNkeylist):\n",
    "        df['petNkey'] = df.apply(lambda x: '+'.join([x.pet_type, x.breed]), axis = 1)\n",
    "    \n",
    "    if 'sales_channel' in df.columns:\n",
    "        df.loc[:, 'sales_channel'].mask(df.loc[:, 'sales_channel'] == 0, inplace = True)\n",
    "        df.loc[:, 'sales_channel'].fillna('Unknown', inplace = True)\n",
    "        df.loc[:, 'sales_channel'] = df.sales_channel.apply(lambda x: x.title())\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sales_channel to claims based on written and policy data\n",
    "\n",
    "scl1 = dataframes_out['written'][['policy_no','sales_channel']].copy()\n",
    "scl2 = dataframes_out['paid'][['policy_no','sales_channel']].copy()\n",
    "\n",
    "scl = pd.concat([scl1, scl2])\n",
    "\n",
    "del scl1, scl2\n",
    "\n",
    "scl.drop_duplicates(subset = ['policy_no'], inplace = True, keep = 'last')\n",
    "\n",
    "scl.dropna(subset = ['policy_no'])\n",
    "\n",
    "dfc = dataframes_out['claims']\n",
    "\n",
    "dfcSC =pd.merge(dfc, scl, how = 'left', on = 'policy_no')\n",
    "dfcSC.loc[:,'sales_channel'].fillna('Unknown', inplace = True)\n",
    "\n",
    "dataframes_out['claims'] = dfcSC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 17\n",
    "step = 50\n",
    "\n",
    "purchase_price_dict = {}\n",
    "\n",
    "purchase_price_buckets = [[-1,'UNKNOWN']]\n",
    "\n",
    "for n in range(steps):\n",
    "    lb = n * step\n",
    "    up = (n + 1) * step - 1\n",
    "    purchase_price_buckets.append([lb, '-'.join([str(lb), str(up)])])\n",
    "\n",
    "purchase_price_buckets.append([purchase_price_buckets[-1][0], str(purchase_price_buckets[-1][0]) + '+'])\n",
    "\n",
    "for x in purchase_price_buckets:\n",
    "    purchase_price_dict[x[0]] = x[1]\n",
    "\n",
    "\n",
    "def fnConvertPurchasePrice(price):\n",
    "    try:\n",
    "        price_int = int(float(price))\n",
    "    except ValueError:\n",
    "        price_int = None\n",
    "\n",
    "    if price_int is not None:\n",
    "        return steps * step if price_int > steps * step else price_int - price_int % step\n",
    "    else:\n",
    "        try:\n",
    "            if \"-\" in str(price):\n",
    "                try:\n",
    "                    return int(re.sub(r'[^0-9]', '',price.split(\"-\")[0]))\n",
    "                except ValueError:\n",
    "                    return -1\n",
    "            else:\n",
    "                return -1\n",
    "        except ValueError:\n",
    "            print(price)\n",
    "\n",
    "def fnLookupPurchaseRange(price):\n",
    "    if price in purchase_price_dict:\n",
    "        return purchase_price_dict[price]\n",
    "    else:\n",
    "        'UNKNOWN'\n",
    "    \n",
    "\n",
    "purchase_price_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['written','paid']:\n",
    "    x = dataframes_out[key]\n",
    "    convertedPrices = \n",
    "    x.loc[:,'purchase_price_bucket_id'] = x.apply(lambda y: fnConvertPurchasePrice(y.purchase_price), axis = 1)\n",
    "    x.loc[:,'purchase_price_bucket'] = x.apply(lambda y: fnLookupPurchaseRange(y.purchase_price_bucket_id), axis = 1)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try match purchase_price to claims data\n",
    "# Tried policyNkey + pet_name. Resulted in 5/12 match\n",
    "# 800 policyNkeys on Claims side have two pet name instances.\n",
    "# If we discard this detail, and use policyNkey, match rate increases to 10/12\n",
    "\n",
    "policy_price_key = pd.DataFrame()\n",
    "for key in ['written','paid']:\n",
    "    y = dataframes_out[key][['policy_no', 'policyNkey', 'purchase_price_bucket_id']].copy()\n",
    "    y = y.dropna(subset = ['policy_no'])\n",
    "    #y['policyNkeyPetName'] = y.apply(lambda z: '+'.join([z.policyNkey, str(z.pet_name)]), axis = 1)\n",
    "    y = y[['policyNkey', 'purchase_price_bucket_id']]\n",
    "    #y.drop_duplicates(subset = ['policyNkeyPetName'], keep = 'first', inplace = True)\n",
    "    y.drop_duplicates(subset = ['policyNkey'], keep = 'first', inplace = True)\n",
    "    policy_price_key = pd.concat([policy_price_key, y])\n",
    "\n",
    "#x.drop_duplicates(subset = ['policyNkeyPetName'], keep = 'first', inplace = True)\n",
    "policy_price_key.drop_duplicates(subset = ['policyNkey'], keep = 'first', inplace = True)\n",
    "del y\n",
    "          \n",
    "policy_price_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes_out['claims']['policyNkeyPetName'] = dataframes_out['claims'].apply(\n",
    "#    lambda z: '+'.join([z.policyNkey, str(z.pet_name)]), axis = 1)\n",
    "if 'purchase_price_bucket_id' not in dataframes_out['claims'].columns:\n",
    "    dataframes_out['claims'] = pd.merge(dataframes_out['claims'], policy_price_key, on = ['policyNkey'], how = 'left')\n",
    "    dataframes_out['claims'].loc[:,'purchase_price_bucket_id'].fillna(-1, inplace = True)\n",
    "    dataframes_out['claims'].loc[:,'purchase_price_bucket'] = dataframes_out['claims']['purchase_price_bucket_id'].apply(lambda y: fnLookupPurchaseRange(y))\n",
    "\n",
    "#list(set(dataframes_out['claims']['purchase_price_bucket_id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cover Type cleaning (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_type = pd.DataFrame()\n",
    "cover_type_add = pd.DataFrame()\n",
    "for key, df in dataframes_out.items():\n",
    "    if 'cover_type' in df.columns:\n",
    "        cover_type_add = df[['affinity_name', 'cover_type']]\n",
    "        cover_type_add = cover_type_add[[(not str(x).startswith('PET')) or str(x).startswith('PETtrac') for x in cover_type_add.cover_type]]\n",
    "        cover_type = cover_type.append(cover_type_add, ignore_index = True)\n",
    "        cover_type.drop_duplicates(inplace = True)\n",
    "\n",
    "#cover_type -= cover_type_remove\n",
    "#cover_type -= set([0, np.NaN, 'UNKNOWN', 'Missing'])\n",
    "\n",
    "cover_type.groupby(by=['affinity_name'], as_index = False)['cover_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_type_dict = {}\n",
    "cover_type_dict = {\n",
    "    'IYP': {\n",
    "        'accident': 'Accident',\n",
    "        'maximum': 'Maximum',\n",
    "        'lifetime': 'Lifetime'\n",
    "    },\n",
    "    'VMC': {\n",
    "        'diamond': 'Diamond',\n",
    "        'gold':'Gold',\n",
    "        'platinum': 'Platinum',\n",
    "        'silver': 'Silver',\n",
    "        '10k': 'Diamond',\n",
    "        '6k': 'Platinum',\n",
    "        '4k': 'Gold',\n",
    "        '2k': 'Silver'\n",
    "    },\n",
    "    'THISTLE': {\n",
    "        '1k': '£1,000',\n",
    "        '3k': '£3,000',\n",
    "        '6k': '£6,000',\n",
    "        '7.5k': '£7,500',\n",
    "        '12k': '£12,000',\n",
    "        '£1,000': '£1,000',\n",
    "        '£3,000': '£3,000',\n",
    "        '£6,000': '£6,000',\n",
    "        '£7,500': '£7,500',\n",
    "        '£12,000': '£12,000'\n",
    "    }\n",
    "}\n",
    "\n",
    "def fnCoverType(affinity, cover_type):\n",
    "    cover_type_key = str(cover_type).lower().strip()\n",
    "    cover_type_desc = 'UNKNOWN'\n",
    "    if affinity in ['IYP','VMC']:\n",
    "        for cover_type_item in cover_type_dict[affinity]:\n",
    "            if cover_type_key.startswith(cover_type_item):\n",
    "                cover_type_desc = cover_type_dict[affinity][cover_type_item]\n",
    "    elif affinity in ['THISTLE']:\n",
    "        for cover_type_item in cover_type_dict[affinity]:\n",
    "            if  cover_type_item in cover_type_key:\n",
    "                cover_type_desc = cover_type_dict[affinity][cover_type_item]\n",
    "    elif affinity == 'RSPCA':\n",
    "        cover_type_desc = cover_type_key.title()\n",
    "    elif affinity == 'LIFETIME':\n",
    "        cover_type_desc = cover_type_key.replace('(legacy)', '').strip().title()\n",
    "\n",
    "    return cover_type_desc\n",
    "\n",
    "cover_type['cover_type_desc'] = cover_type.apply(lambda x: fnCoverType(x.affinity_name, x.cover_type), axis = 1)\n",
    "#cover_type.head()\n",
    "\n",
    "\n",
    "cover_type_lookup = {}\n",
    "cover_type_lookup_item = {}\n",
    "\n",
    "for affinity in set(cover_type.affinity_name):\n",
    "    for ct in set(cover_type[cover_type.affinity_name == affinity]['cover_type'].tolist()):\n",
    "        cover_type_lookup[affinity][cover_type_lookup_item[ct]] = fnCoverType(affinity, ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'10K'.startswith('10K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcode\n",
    "\n",
    "#### Claims - Add from CSV lookup\n",
    "Customer postcode is not included in the claims data\\\n",
    "Could be joined from policy data or retrieved from Aquarius \\\n",
    "Aquarius seems better\n",
    "\n",
    "#### Written/Paid - Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get claim_id, postcode data from CSV (SQL output from AQ as postcode is not available in AllClaims data)\n",
    "directory = '\\\\\\\\sigltd.co.uk\\SIG_net\\Departments\\Protection\\Pet\\Loss Ratios\\MDA File prep'\n",
    "filename = 'AllClaimsPostcode.csv'\n",
    "\n",
    "allClaimsPostcode = pd.read_csv(os.path.join(directory, filename), header = None)\n",
    "allClaimsPostcode.columns = ['claim_id','post_code']\n",
    "\n",
    "allClaimsPostcode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex =re.compile(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})')\n",
    "\n",
    "postcodeErrors = {\n",
    "    'DT4OPL':'DT4 0PL',\n",
    "    'WD5OGE': 'WD5 0GE'\n",
    "}\n",
    "def postcodeRegEx(postcode, errorValue = 'INVALID'):\n",
    "    if pd.isna(postcode):\n",
    "        return 'UNKNOWN'\n",
    "    postcode = str(postcode).replace('-', '').replace(' ','').upper()\n",
    "    if postcode == 'IRELAND':\n",
    "        'IRELAND'\n",
    "    matchobj = regex.search(postcode)\n",
    "    if matchobj:\n",
    "        matchobj = matchobj.group(0).replace(\" \", \"\")\n",
    "        return matchobj[:-3] + ' ' + matchobj[-3:]\n",
    "    elif postcode in postcodeErrors:\n",
    "        return postcodeErrors[postcode]\n",
    "    return errorValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge AllClaimsPostcode into Claims dataframe\n",
    "if 'post_code' not in dataframes_out['claims'].columns:\n",
    "    claims = dataframes_out['claims']\n",
    "    claims = pd.merge(claims, allClaimsPostcode, on = 'claim_id', how = 'left')\n",
    "    dataframes_out['claims'] = claims\n",
    "\n",
    "dataframes_out['claims'][['claim_id','post_code']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge AllClaimsPostcode into Claims dataframe\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "\n",
    "for key, df in dataframes_out.items():\n",
    "    if 'post_code' in df.columns:\n",
    "        df.loc[:, 'post_code_orig'] = df.post_code\n",
    "        df.loc[:, 'post_code'] = df.post_code.apply(lambda x: postcodeRegEx(x))\n",
    "        df.loc[:, 'post_code_orig'] = df.apply(lambda x: x.post_code_orig if x.post_code == 'INVALID' else x.post_code, axis = 1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save each table\n",
    "for key, df in dataframes_out.items():\n",
    "    print(key + '.csv')\n",
    "    write_df_to_csv(df, key + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcodes\n",
    "Experian UK 7 data. Creates dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '\\\\\\\\coveainsurance.co.uk\\data\\Shared Data\\BI Reporting\\BI Function\\Power BI\\\\2. Offline Data Sources'\n",
    "\n",
    "pc_extract = pd.read_csv(os.path.join(directory, 'PC_Extract.csv'))\n",
    "pc_type = pd.read_csv(os.path.join(directory, 'PC_Types.csv'))\n",
    "pc_group = pd.read_csv(os.path.join(directory, 'PC_Groups.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_extract.columns = ['post_code_equivalent','post_code_flag','post_code','group_id','type_id']\n",
    "pc_extract.loc[:,'group_key'] = pc_extract.apply(lambda x: '+'.join([x.group_id, str(x.type_id) ]), axis = 1)\n",
    "pc_extract.head()\n",
    "len(pc_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnSplitPostcode(postcode):\n",
    "    postcode = postcode.replace(' ','')\n",
    "    return postcode[:-3] + ' ' + postcode[-3:]\n",
    "\n",
    "pc_extract.loc[:,'post_code'] = pc_extract.post_code.apply(lambda x: fnSplitPostcode(x))\n",
    "pc_extract['district'] = pc_extract.post_code.apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "regex = re.compile(r'[^A-Z]')\n",
    "pc_extract['post_code_area'] = pc_extract.district.apply(lambda x: regex.sub('',x[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing postcodes from Claims, Written and Paid\n",
    "pc_missing = dataframes_out['claims'].post_code.tolist()\n",
    "pc_missing += dataframes_out['paid'].post_code.tolist()\n",
    "pc_missing += dataframes_out['written'].post_code.tolist()\n",
    "pc_missing = set(pc_missing)\n",
    "pc_missing -= set(pc_extract.post_code)\n",
    "pc_missing -= set(['INVALID','UNKNOWN'])\n",
    "print(len(pc_missing))\n",
    "cols = ['post_code_equivalent','post_code_flag','post_code','group_id','type_id','group_key','district','post_code_area']\n",
    "\n",
    "df_missing = []\n",
    "for pc in pc_missing:\n",
    "    d = pc.split(' ')[0]\n",
    "    a = regex.sub('', d[:2])\n",
    "    l = [pc,None,pc,'U','99','U+99', d, a]\n",
    "    df_missing.append(l)\n",
    "    \n",
    "df_missing = pd.DataFrame(df_missing, columns = cols)\n",
    "pc_extract = pc_extract.append(df_missing, ignore_index = True)\n",
    "print(len(pc_extract))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_groups = pc_extract[['group_id','type_id']].drop_duplicates()\n",
    "if 'group_desc' not in pc_groups.columns:\n",
    "    pc_groups = pd.merge(pc_groups, pc_group, on = 'group_id', how = 'left')\n",
    "    pc_groups = pd.merge(pc_groups, pc_type, on = 'type_id', how = 'left')\n",
    "pc_groups.loc[:,'group_key'] = pc_groups.apply(lambda x: '+'.join([x.group_id, str(x.type_id)]), axis = 1)\n",
    "pc_groups.drop(columns = ['group_id','type_id'], inplace = True)\n",
    "pc_groups = pc_groups.drop_duplicates(subset = ['group_key'])\n",
    "pc_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_areas_xls = pd.ExcelFile(os.path.join(directory, 'PostcodeDistricts.xlsx'))\n",
    "pc_areas_xls.sheet_names\n",
    "\n",
    "dfDistrict = pd.read_excel(pc_areas_xls, sheet_name='Districts', header=0)\n",
    "dfDistrict.drop(columns = ['DistrictCount','Area'], inplace = True)\n",
    "dfDistrict.columns = ['district','county']\n",
    "\n",
    "dfArea = pd.read_excel(pc_areas_xls, sheet_name = 'Area', header=0)\n",
    "dfArea.columns = ['post_code_area','post_code_area_name','post_code_region']\n",
    "\n",
    "dfArea.head()\n",
    "#dfDistrict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pc_extract))\n",
    "if 'county' not in pc_extract.columns:\n",
    "    pc_extract = pd.merge(pc_extract, dfDistrict, on = 'district', how = 'left')\n",
    "    \n",
    "if 'post_code_area_name' not in pc_extract.columns:\n",
    "    pc_extract = pd.merge(pc_extract, dfArea, on = 'post_code_area', how = 'left')\n",
    "print(len(pc_extract))    \n",
    "\n",
    "pc_extract.drop(columns = ['post_code_equivalent','post_code_flag'], inplace = True)\n",
    "\n",
    "pc_extract = pd.merge(pc_extract, pc_groups, on = 'group_key', how = 'left')\n",
    "pc_extract.rename(columns = {'group_desc': 'UK7_group_desc', 'type_desc': 'UK7_type_desc'}, inplace = True)\n",
    "\n",
    "pc_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_u = pd.DataFrame([\n",
    "    ['UNKNOWN', 'U', 99, 'U+99', 'UNKNOWN', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown'],\n",
    "    ['INVALID', 'U', 99, 'U+99', 'INVALID', 'Invalid', 'Invalid', 'Invalid', 'Invalid', 'Invalid', 'Invalid']\n",
    "], columns = pc_extract.columns)\n",
    "\n",
    "\n",
    "pc_extract = pc_extract.append(pc_u, ignore_index = True)\n",
    "pc_extract[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_extract = pc_extract.fillna('Unknown')\n",
    "write_df_to_csv(pc_extract, 'postcodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
